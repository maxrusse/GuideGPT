{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f68adb6-b6de-47ea-9421-4947decc68e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Documents loaded.\n",
      "Building Vector Store Index...\n",
      "Vector Store Index created.\n",
      "Persisting index to directory: INDEX\n",
      "Index persisted successfully.\n"
     ]
    }
   ],
   "source": [
    "### Code written based on LLAMA_INDEX v0.5.0 and OPENAI shortly after release of GPT4\n",
    "### for using with recent Versions of OpenAI and LLAMA_Index - Major Code-Changes are to be expected!\n",
    "#Version used for this code:\n",
    "# PyPDF2 (PyPDF2): 3.0.1 [Pip]\n",
    "# langchain (langchain): 0.0.332 [Pip]\n",
    "# llama-index (llama_index): 0.8.65 [Pip]\n",
    "# nest_asyncio (nest_asyncio): 1.5.8 [Pip]\n",
    "# openai (openai): 1.2.0 [Pip]\n",
    "# pypdf (pypdf): 3.15.1 [Pip]\n",
    "\n",
    "\n",
    "# ### Cell 1: Building and Persisting the Vector Store Index\n",
    "\n",
    "# Import Necessary Libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import nest_asyncio  \n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import PyPDF2\n",
    "\n",
    "# Importing classes and functions from llama_index library for indexing and querying\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Importing ChatOpenAI from langchain for language model interactions\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai  # OpenAI API client\n",
    "\n",
    "# Apply Nest Asyncio\n",
    "# This allows the Jupyter notebook to handle asynchronous operations properly.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration and Environment Setup\n",
    "\n",
    "# Security Note: It's crucial to manage API keys securely.\n",
    "# Avoid hardcoding them in your scripts. Consider using environment variables or\n",
    "# a dedicated secrets manager.\n",
    "#os.environ[\"OPENAI_API_KEY\"] = 'sk-...your-api-key...'  # Replace with your actual API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Fetch the API key from environment variables\n",
    "\n",
    "# Logging Configuration\n",
    "# Uncomment the following lines to enable logging for debugging purposes.\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Define Constants\n",
    "INPUT_DIRECTORY = 'MRONJ'  # Directory containing your input documents (e.g., PDFs)\n",
    "PERSIST_DIR = \"INDEX\"      # Directory where the index will be saved for persistence\n",
    "\n",
    "# LLM (Language Model) Configuration Parameters\n",
    "# Embedding model ist default to text-embedding-ada-002\n",
    "LLM_MODEL_NAME = \"gpt-4\"  # Choose between models like \"gpt-3.5-turbo\" or \"gpt-4\"\n",
    "LLM_TEMPERATURE = 0.6              # Controls the randomness of the model's output\n",
    "CONTEXT_WINDOW_SIZE = 4096          # Maximum tokens the model can handle in context\n",
    "CHUNK_SIZE = 1024                   # Size of text chunks for processing\n",
    "EMBED_BATCH_SIZE = 150              # Batch size for embedding generation\n",
    "\n",
    "# Function to Attach Filename Metadata to Documents\n",
    "def attach_filename_metadata(filename):\n",
    "    \"\"\"\n",
    "    Attaches the filename as metadata to each document.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the filename metadata.\n",
    "    \"\"\"\n",
    "    return {'file_name': filename}\n",
    "\n",
    "# Initialize LLMPredictor with ChatOpenAI\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=ChatOpenAI(temperature=LLM_TEMPERATURE, model_name=LLM_MODEL_NAME)\n",
    ")\n",
    "\n",
    "# Create a Sentence Window Node Parser with Default Settings\n",
    "# This parser splits documents into overlapping sentences (windows) for better context handling.\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,  # Number of sentences in each window\n",
    "    window_metadata_key=\"window\",  # Metadata key for window information\n",
    "    original_text_metadata_key=\"original_text\",  # Metadata key for original text\n",
    ")\n",
    "\n",
    "# Set Up the Service Context\n",
    "# The ServiceContext combines the LLM predictor, embedding model, and node parser.\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=OpenAIEmbedding(embed_batch_size=EMBED_BATCH_SIZE),\n",
    "    node_parser=node_parser,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    context_window=CONTEXT_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# Load Documents and Build Index\n",
    "\n",
    "print('Loading documents...')\n",
    "# Reads documents from the specified input directory and attaches filename metadata\n",
    "documents = SimpleDirectoryReader(INPUT_DIRECTORY, file_metadata=attach_filename_metadata).load_data()\n",
    "print('Documents loaded.')\n",
    "\n",
    "print('Building Vector Store Index...')\n",
    "# Creates a VectorStoreIndex from the loaded documents using the service context\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "print('Vector Store Index created.')\n",
    "\n",
    "print(f'Persisting index to directory: {PERSIST_DIR}')\n",
    "# Saves the index to the specified persistence directory for later use\n",
    "index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "print('Index persisted successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36bf141-d138-4df6-9aa2-f7ffa27d3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "- Answer: The use of medications with antiangiogenic properties such as glucocorticoids, thalidomide, and bortezomib may increase the risk of MRONJ in patients under antiresorptive therapy. However, it is important to note that other factors, such as underlying dental problems and dental procedures, have been identified as significant risk factors for MRONJ.\n",
      "\n",
      "- Explanation: Medications with antiangiogenic properties, such as glucocorticoids, thalidomide, and bortezomib, have been identified as coexisting factors that may increase the risk of MRONJ in patients undergoing antiresorptive therapy. These drugs can impair the body's ability to repair and remodel bone, particularly in an environment that is trauma-intense and bacteria-laden, such as the oral cavity. This can lead to the development of osteonecrosis. However, research has shown that underlying dental problems, such as infection or dental extraction, were present in a significant percentage of patients who developed MRONJ. Therefore, while the use of certain medications may increase the risk, dental health and procedures are also significant factors to consider. As such, patients receiving antiresorptive therapy should be carefully monitored and dental health should be maintained to minimize the risk of developing MRONJ.\n",
      "\n",
      "Source Files and Pages:\n",
      "2005 - Osteonecrosis of the Jaw and Bisphosphonates.pdf Page: (1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 2: Querying the Vector Store Index\n",
    "\n",
    "# Import Necessary Libraries for Application\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import PyPDF2\n",
    "\n",
    "# Importing additional classes and functions from llama_index for querying and post-processing\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    QuestionAnswerPrompt,\n",
    "    RefinePrompt\n",
    ")\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai  # OpenAI API client\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Apply nest_asyncio again if not already applied\n",
    "\n",
    "# Configuration and Environment Setup\n",
    "\n",
    "# Security Note: Ensure API keys are managed securely.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-...your-api-key...'  # Replace with your actual API key securely\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Fetch the API key from environment variables\n",
    "\n",
    "# Logging Configuration\n",
    "# Uncomment the following lines to enable logging for debugging purposes.\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Define Constants\n",
    "PERSIST_DIR = \"INDEX\"  # Directory where the index is persisted\n",
    "\n",
    "# LLM Configuration Parameters for Querying\n",
    "# Embedding model ist default to text-embedding-ada-002\n",
    "QUERY_MODEL = \"gpt-4\"                # Choose between models like \"gpt-3.5-turbo\" or \"gpt-4\"\n",
    "QUERY_TEMPERATURE = 0.4               # Controls the randomness of the model's output\n",
    "QUERY_CONTEXT_WINDOW = 8192           # Maximum tokens the model can handle in context for queries\n",
    "QUERY_NUM_OUTPUT_TOKENS = 1024        # Number of tokens the model should generate in response\n",
    "SIMILARITY_TOP_K = 20                 # Number of top similar documents to retrieve\n",
    "RESPONSE_MODE = \"compact\"             # Mode for the response formatting\n",
    "API_TIMEOUT = 120                     # Timeout for API requests in seconds\n",
    "\n",
    "# Function to Load the Existing Index from Storage\n",
    "def load_existing_index(persist_dir):\n",
    "    \"\"\"\n",
    "    Loads the existing index from the specified persistence directory.\n",
    "\n",
    "    Args:\n",
    "        persist_dir (str): The directory where the index is persisted.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The loaded index.\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading index from storage directory: {persist_dir}')\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    logging.info('Index loaded successfully.')\n",
    "    return index\n",
    "\n",
    "# Define Prompt Templates\n",
    "\n",
    "# Template for the initial question-answering prompt in English\n",
    "ENGLISH_QA_PROMPT_TEMPLATE = (\n",
    "    \"We have provided scientific context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please answer the following question: {query_str}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response as part of a Clinical Practice Guidelines for MRONJ:\\n\"\n",
    "    \"- Answer: (Provide a concise yet comprehensive and self-contained answer summarizing the key points.)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Explanation: (Explain your answer by providing informative yet concise details. Describe relevant factors or actions and outline the impact.)\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "\n",
    "# Initialize the QuestionAnswerPrompt with the defined template\n",
    "ENGLISH_QA_PROMPT = QuestionAnswerPrompt(ENGLISH_QA_PROMPT_TEMPLATE)\n",
    "\n",
    "# Template for refining the initial answer based on additional context\n",
    "ENGLISH_REFINE_PROMPT_TEMPLATE = (\n",
    "    \"The original question is as follows: {query_str}\\n\"\n",
    "    \"We have provided an original answer: {existing_answer}\\n\"\n",
    "    \"We have the option to refine the original answer (only if necessary) with some more context below.\\n\"\n",
    "    \"If necessary, try to improve the answer and explanation by adding more details.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the question. The answer might be changed or adopted based on new context.\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response as part of a Clinical Practice Guidelines for MRONJ:\\n\"\n",
    "    \"- Answer: (Provide a concise yet comprehensive and self-contained answer summarizing the key points. Modify based on new context if applicable.)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Explanation: (Explain your answer by providing informative yet concise details. Describe relevant factors or actions and outline the impact) Modify based on new context if applicable.\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"If the new context is not useful, repeat exactly the original answer.\\n\"\n",
    ")\n",
    "\n",
    "# Initialize the RefinePrompt with the defined template\n",
    "ENGLISH_REFINE_PROMPT = RefinePrompt(ENGLISH_REFINE_PROMPT_TEMPLATE)\n",
    "\n",
    "# Function to Initialize the Query Engine\n",
    "def initialize_query_engine(index):\n",
    "    \"\"\"\n",
    "    Initializes the query engine with the specified index and configuration.\n",
    "\n",
    "    Args:\n",
    "        index (VectorStoreIndex): The vector store index to query against.\n",
    "\n",
    "    Returns:\n",
    "        QueryEngine: The initialized query engine.\n",
    "    \"\"\"\n",
    "    # Initialize the LLMPredictor with ChatOpenAI for querying\n",
    "    query_llm_predictor = LLMPredictor(\n",
    "        llm=ChatOpenAI(temperature=QUERY_TEMPERATURE, model_name=QUERY_MODEL)\n",
    "    )\n",
    "    \n",
    "    # Set up the Service Context for querying\n",
    "    query_service_context = ServiceContext.from_defaults(\n",
    "        llm_predictor=query_llm_predictor,\n",
    "        chunk_size=QUERY_NUM_OUTPUT_TOKENS,\n",
    "        context_window=QUERY_CONTEXT_WINDOW\n",
    "    )\n",
    "    \n",
    "    # Initialize the Query Engine with specified parameters and prompts\n",
    "    query_engine = index.as_query_engine(\n",
    "        service_context=query_service_context,\n",
    "        similarity_top_k=SIMILARITY_TOP_K,\n",
    "        response_mode=RESPONSE_MODE,\n",
    "        text_qa_template=ENGLISH_QA_PROMPT,\n",
    "        refine_template=ENGLISH_REFINE_PROMPT,\n",
    "        node_postprocessors=[\n",
    "            MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Function to Extract Filenames and Pages from Response Metadata\n",
    "def get_filenames(response):\n",
    "    \"\"\"\n",
    "    Extracts and summarizes filenames and their corresponding pages from the query response.\n",
    "\n",
    "    Args:\n",
    "        response (Response): The response object from the query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string summarizing source files and pages.\n",
    "    \"\"\"\n",
    "    filenames_pages = dict()\n",
    "    for node in response.source_nodes:\n",
    "        metadata = node.node.metadata\n",
    "        filename_label = metadata.get(\"file_name\")  # Retrieve filename from metadata\n",
    "        page_label = metadata.get(\"page_label\")    # Retrieve page number from metadata\n",
    "        if filename_label is not None:\n",
    "            filename_label = os.path.basename(filename_label)  # Extract base filename\n",
    "            if filename_label not in filenames_pages:\n",
    "                filenames_pages[filename_label] = set()\n",
    "            if page_label is not None:\n",
    "                filenames_pages[filename_label].add(f\"{page_label}\")\n",
    "    summary = []\n",
    "    for filename, pages in filenames_pages.items():\n",
    "        # Sort pages numerically if possible\n",
    "        sorted_pages = sorted(pages, key=lambda x: (not x.isdigit(), int(x) if x.isdigit() else x))\n",
    "        summary.append(f\"{filename} Page: ({', '.join(sorted_pages)})\" if pages else filename)\n",
    "    return \", \".join(summary)\n",
    "\n",
    "# Function to Execute a Query and Display Results with Filename Metadata\n",
    "def execute_query(query_engine, query_text):\n",
    "    \"\"\"\n",
    "    Executes a query using the provided query engine and displays the response along with source metadata.\n",
    "\n",
    "    Args:\n",
    "        query_engine (QueryEngine): The query engine to use for executing the query.\n",
    "        query_text (str): The text of the query to execute.\n",
    "    \"\"\"\n",
    "    logging.info('Executing query...')\n",
    "    response = query_engine.query(query_text)\n",
    "    logging.info('Query executed successfully.')\n",
    "    \n",
    "    # Display the Response\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    \n",
    "    # Extract and Display Filenames and Pages\n",
    "    filenames_summary = get_filenames(response)\n",
    "    print(\"\\nSource Files and Pages:\")\n",
    "    print(filenames_summary)\n",
    "\n",
    "# Load the Existing Index from Storage\n",
    "index = load_existing_index(PERSIST_DIR)\n",
    "\n",
    "# Initialize the Query Engine\n",
    "query_engine = initialize_query_engine(index)\n",
    "\n",
    "# Define the Sample Query\n",
    "sample_query = 'Which concomitant medications (drug classes) increase the risk of MRONJ in patients under antiresorptive therapy?'\n",
    "\n",
    "# Execute the Query and Display Results\n",
    "execute_query(query_engine, sample_query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_old",
   "language": "python",
   "name": "openai_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
